Flow-Guided Feature Aggregation for Video Object Detection  (FGFA)

数据集:

ImageNet VID dataset

数据输入:

视频帧 以及前后 K 帧 (默认取 K = 10)

视频帧之间的光流通过 FlowNet-simple 计算

网络结构：

Resnet 101 当前帧以及前后k帧的feature map

FlowNet 前后K帧对于当前帧的光流

Flow-guided warping 根据前后K帧的光流将前后K帧的feature map 投影到当前帧的feature map上

Feature aggregation embedding 计算前后K帧的Feature map 对于当前帧的feature map投影的权重，并作融合

rpn-layer 举荐候选框  (底层网络为R-FCN)

Dense-layer 分类，回归

核心点：


A)

Flow-guided warping

通过第i,j帧原图之间的光流 (通过FlowNet计算) 来实现把第j帧的Feature map 通过原图计算生成的光流场作为参考投射(wrap)到第i帧的feature map上

投射函数W:

通过mx.symbol.GridGenerator 来得到 (光流作为输入) 2D sampling grid 

mx.symbolGridGenerator 有两种输入形式，一种是仿射变换(affine) 另外一种为光流(warp)
这里选择光流作为输入形式

接着利用mx.sym.BilinearSample 来实现位置的精确插入 (2D sampling grid 和feature map作为输入, 因为光流场的大小为原图的尺寸，而feature map为卷积后的结果 所以需要BilinearSample 实现光流场对feature map 的对应)  实现通过光流把j帧的feature map投射到第i帧的feature map上

GridGenerator  以及 BilinearSample 实现wrap的详细解释：
该函数主要输入一个形状为(N,C,Hin,Win)的input张量，输入一个形状为(N,Hout,Wout,2)的grid张量，输出一个形状为(N,C,Hout,Wout)的output张量。

输入的grid是一个Hout × Wout大小的空间位置矩阵，其中每个元素都代表着一个二维空间坐标(x,y)，该坐标指明了在input上采样的坐标,而输出张量的每个位置output[n,:,h,w]的值，取决于这个输入input和采样坐标的值（通过双线性插值形成）


B)
Feature aggregation
对于每一帧j(i-k <= j < i+k) 计算第j帧的feature map 对第i帧 feature map的投影 
具体为计算第j帧原图对第i帧原图的光流场,并以此为参考将j帧的feature map wrap至第i帧的feature map 上
得到了前后K帧的feature map对当前帧feature map投影 Ej-->i(i-k <= j < i+k)后，由一个Embedding 网络分别计算他们各自的Embedding feature Fj-->i，以及当前帧本身的Embedding Fi   即Embedding 的详细步骤为 对于每一帧j(i-k <= j < i+k) 输入 Ej-->i  计算 Fj-->i  以及 输入Ei 计算 Fi
Embedding 网络由三层卷积层构成

接着对于每一帧j(i-k <= j < i+k) 计算第j帧的feature map对当前帧feature map投影的权重 (利用输出的Embedding)
计算公式为：Wj-->i = exp((Fj-->i dot Fi) / Abs(Fj-->i) * Abs(Fi))

分别得出前后K帧feature map对于当前帧feature map投影的权重后，再分别由对应的前后K帧的feature map对当前帧feature map投影乘他们各自的权重，再结合当前帧进行总体相加融合



现阶段由于只有Colab, 以及云端环境，暂时无法完成训练

Github link:
https://github.com/msracver/Flow-Guided-Feature-Aggregation


知识点延伸:

A)

稀疏光流：关键点的光流
稠密光流：所有像素点点运动光流

FlowNet simple
将需要计算光流的两张图片拼接在一起(w * h * 6)

网络有两部分组成：卷积层和 refinement net

卷积层为8-9个conv2d 和 maxplooing 组成

refiement net 则有5层

最后4层每层都会 1）结合refinement net前一层网络解卷积的结果，
2）以及第一部分卷积网络对应卷积层的卷积结果，3）以及refinement net前一层预测光流(并上采样)的结果
拼接而成



B)

Spatial Transformer Networks (STN) 仿射变换网络

输入：原图， label: 仿射变换后的图片（校正过后的图片）

训练值：仿射变换矩阵

由三部分组成

Localisation net 

Grid Generator

Sampler

图片的仿射变换可通过一个3*2的矩阵实现 

原图任意点坐标 x1,y1  ==>  (x1,y1,1)^T 变换矩阵 (a,b,c ; d,e,f)  任意点新坐标(x2,y2)

(x2,y2)^T = (a,b,c ; d,e,f) (x1,y1,1)^T 

在 Localisation net中，图片经过两层Conv + maxplooling 以及两层Dense 生成一个3*2的变换矩阵

在Grid Generator 中根据3*2变换矩阵产生2D sampling grid 

在Sampler中完成转换 (BilinearSample)








